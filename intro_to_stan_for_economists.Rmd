---
title: "An introduction to Bayesian modelling in Stan for economists"
author: "Jim Savage"
date: "20 February 2016"
output: html_document
---

Stan is a flexible modelling language that makes it straightforward to estimate a very wide range of probability models using Bayesian techniques. There are a few reasons one may want to spend the time to learn Stan: 

- Stan implements efficient estimation of probability models on large data sets using Hamiltonain Monte Carlo, Variational Inference and Penalised Maximum Likelihood. Stan can be called from many environments that users may use for data preparation, including R, Python and Julia. 
- Stan allows users to speed up their R code by exporting Stan functions (which are compiled C++ functions) into R. This is especially useful for users who want similar performance to Julia but are tied to R for its large library ecosystem.
- Stan is perhaps the easiest Bayesian library to learn and use, with straightforward syntax and companion libraries for easy model checking and comparison. 
- Stan (alongside the workflow that is encouraged by the Stan community) forces users to think more carefully about their models than they may do otherwise. Learning Stan illuminates many aspects of statistics that you may have skipped over; it should help you think more carefully about modelling even if you do not use Stan. 
- In Stan there is no penalty from using non-conjugate prior distributions, allowing far richer model specifications than under other MCMC methods. 

By the end of this tutorial, you should feel comfortable with the following: 

1. Understand the Bayesian workflow 
2. Know how to write out a Stan model
3. Know how to estimate and check a model in R
4. Know where to get help

The examples given below will use R's interface with Stan, `rstan`. While Stan can also be called from Python (using `PyStan`), Julia (using `Stan.jl`) and other environments, there are some useful companion libraries that are better developed in R. 


## 1. The Bayesian Workflow

A strong culture exists within the Bayesian community around a workflow that promotes high quality modelling. Many of the steps should be familiar to economists, yet a few are distinct to Bayesian modelling.

   - writing out the probability model
   - simulating the model with known parameters
   - estimating the model to recover the parameters from simulated data
   - estimating the model against real data
   - check that the estimation has run properly
   - run posterior predictive checking/time series cross validation to evaluate model fit
   - perform inference and prediction 
   - model comparison

While at first it may seem a drag to adhere to this workflow strictly, after a while it should feel natural. It will reduce the number of errors in your work, and help you think through the details of the modelling task. Let's walk through each step, gradually introducing Stan along the way. 

### A) Writing out your model in probability form

Stan estimates probability models. These are models in which we assume that all unknown parameters and the outcome variable(s) $y$ each come from some (conditional) probability distributions. 

As a first example, take the linear regression model. In this model we have an ($N$-long) vector of outcomes $y$, a matrix of covariates $X$ ($N\times P$), a ($P$-long) vector of unknown coefficients $\beta$, and a ($N$-long) vector of residuals $\epsilon$. The model is typically written out in matrix notation as

$$
y = X\beta + \epsilon
$$

This is not yet a probability model. To express it as a probability model, we make an assumption about the distribution of the residuals and parameters. We'll start by assuming the residuals are normally distributed $\epsilon \sim \mathcal{N}(0, \sigma)$, where $\sigma$ is another parmeter--the standard deviation of the residuals. 

Once we have made this distributional assumption, we can use it to express the linear model above in probability notation.

$$
y = X\beta + \epsilon \mbox{ and } \epsilon \sim \mathcal{N}(0, \sigma) \implies y \sim \mathcal{N}(X\beta, \sigma)
$$

(This follows from the fact that if we add $X\beta$ to $\epsilon$, we have a new distribution with a mean of $X\beta$.) 

Note that the normal distribution $\mathcal{N}(\mu, \sigma)$ is a _two-parameter_ distribution. It has a mean $\mu$ and standard deviation $\sigma$. This does not stop us from estimating a model that has a function for each parameter that itself may have several parameters---in the case above, we're using a linear function $\mu = X\beta$ (with $P$ regression coefficients in $\beta$) for the mean model.

The model $y \sim \mathcal{N}(X\beta, \sigma)$ is the _data model_. If we knew the value of the parameters $\beta$ and $\sigma$ for certain, we could simulate plausible values for an outcome $\hat{y_{i}}$ for a given set of covariates $X_{i}$ simply by generating random numbers drawn from $\mathcal{N}(X_{i}\beta, \sigma)$. (This gets to an important aspect of Bayesian models: there is no _predicted value_, rather a _predictive distribution_). Yet we don't know $\beta$ and $\sigma$ for certain, and want to generate probabilistic estimates for them. 

In Bayesian statistics we try to learn about a _posterior_ distribution, in this case $p(\beta, \sigma | y, X)$---a joint distribution of the parameters given the data. According to Bayes' law, this distribution will be proportional to the product of the Likelihood and prior distributions given to the parameters. That is

$$
p(\beta, \sigma | y, X) \propto p(y | \beta, \sigma, X)\times p(\beta, \sigma)
$$

If we assume that the prior distributions of $\beta$ and $\sigma$ are independent, then $p(\beta, \sigma) = p(\beta)\times p(\sigma)$, and this becomes

$$
p(\beta, \sigma | y, X) \propto p(y | \beta, \sigma, X)\times p(\beta)\times p(\sigma)
$$


### Side-note on likelihood

This section contains an explanation of likelihood; feel free to skip over if you feel comfortable with the concept already. 

The term $p(y | \beta, \sigma)$ is the *likelihood*. It is the conditional probability of observing the data $y$ given that the parameters $\theta = \{\beta, \sigma\}$ and our model are "true". A different set of parameters will in general result in a different likelihood. 




To calculate the likelihood we must also make an assumption that each $\epsilon_{i}$ is independent of all other draws. This is equivalent to saying that a draw from $y$, $y_{i}$ is independent of other draws $y_{j}$ after we account for the differences between $X_{i}$ and $X_{j}$. After making this independence assumption, then the  a and is calculated as the product of the liklihood contributions of all observations $y_{i}$, $\prod_{i} p(y_{i}| \beta, \sigma)$. What do these likelihood contributions mean? 

Say 

We already made a modelling assumption about $p(y | \beta, \sigma, X)$--it is our data model $\mathcal{N}(X_{i}\beta, \sigma)$. Now we just need to specify priors for the parameters $\beta$ and $\sigma$. 

For regression coefficients, we often use univariate or multivariate normal priors (we will use univariate priors below). The mean and scale of the prior distributions can be used to include information that is known before observing the data, such as parameter estimates from a meta-study, or a theoretically imposed value. It is also common to use so-called _shrinkage priors_ or _regularising priors_. These are priors that _shrink_ the estimate towards 0 (or a group mean). In many prediction tasks, shrinkage priors help prevent overfitting.

An important note on prior distributions: if a prior distribution puts no weight on a particular value of the parameter, the posterior distribution cannot put weight on that value either. We can use this useful property to use prior distributions in order to restrict estimates of $\beta$ to economically meaningful values. For instance, if we are estimating a very simple cost function $\mbox{costs} = \alpha + \beta \mbox{quantity sold} + e$, we may want to exclude the possibility of zero or negative fixed costs (that is, we think that $\alpha> 0$). In such a case, we could give $\alpha$ a prior distribution that is only defined for positive values, such as the lognormal distribution, or a gamma distribution. 

Similarly, the prior for $\sigma$ should be constrained to positive numbers. A convenient prior for standard deviations is the half Cauchy distribution (restricted to positive numbers), which provides some prior information but allows for potentially large standard deviations. 

#### Putting it all together

We now have our probability model. It is made up of two prior distributions for the parameters (one for the $\beta$s, one for the $\sigma$), and a model for the data given the parameters. 

Where $\mu_{p}, \sigma_{p}, x_{0}\mbox{ and } \gamma$ summarise prior information and are chosen by the researcher, we have the priors: 

$$
\mbox{for }p\in [1 \dots P] \mbox{    }\beta_{p} \sim\mathcal{N}(\mu_{p}, \sigma_{p})
$$

$$
\sigma \sim\mathcal{Cauchy}_{+}(x_{0}, \gamma)
$$

And the data model

$$
y \sim\mathcal{N}(X\beta, \sigma)
$$


#### A slightly richer model

The illustration above shows _how_ you could put together a simple probability model, but does not motivate _why_ you might want to. The real power of probabilistic modelling is that it is not much harder to define far richer models than simple ones. 

For instance, let's say that we want to make two changes to the model above. First, we'd like to accept that our outcome $y$ might come from a so-called "fat tail" distribution like the student's t distribution. Next, we want to restrict the first element of $\beta$ to be non-negative. We can define this slightly richer model like so: 

Priors: 

$$
\beta_{1} \sim\mathcal{N}_{+}(\mu_{1}, \sigma_{1})
$$

$$
\mbox{for }p\in [2 \dots P] \mbox{    }\beta_{p} \sim\mathcal{N}(\mu_{p}, \sigma_{p})
$$

$$
\sigma \sim\mathcal{Cauchy}_{+}(x_{0}, \gamma)
$$

And the data model

$$
y \sim\mbox{Student's t}(\nu, X\beta, \sigma)
$$

Where $\nu$ is the degrees of freedom of the student's t distribution. We need to give this parameter a prior also, limited below by 1. As the student's t distribution approaches a normal distribution as $\nu$ gets much above 20, we may want to centre our distribution around 7, with a fairly wide spread. 

$$
\nu \sim \mathcal{Cauchy}_{>1}(7, 10)
$$



### B) Simulating the model with known parameters (an introduction to Stan)

Now that we have two models, we should generate some simulated values of $y$ for known parameters $\theta = \{\beta, \sigma, \nu\}$ and covariates $X$. While this would be trivial to do in your statistics package of choice, we'll use this task as an opportunity to introduce Stan. 

A Stan model is comprised of code blocks. Each block is a place for a certain task. The bold blocks below must be present in all Stan programs (even if they contain no arguments): 

1. `functions`, where we define functions to be used in the blocks below
2. **`data`**, declares the data to be used for the model
3. `transformed data`, makes transformations of the data passed in above
4. **`parameters`**, defines the unknowns to be estimated, including any restrictions on their values. 
5. `transformed parameters`, often it is preferable to work with transformations of the parameters and data declared above; in this case we define them here. 
6. **`model`**, where the full probability model is defined. 
7. `generated quantities`, generates a range of outputs from the model (posterior predictions, forecasts, values of loss functions, etc.). 

Let's write a toy model to generate simulated data.

First we need to load some libraries. I normally use `dplyr` for data munging, `ggplot2` for plotting the parameters, `rstan`, which is R's interface with Stan, and `reshape2`, which I use to manipulate parameter draws. 

```{r, cache = T, message = F, warning=FALSE}

# Load necessary libraries and set up multi-core processing for Stan

library(dplyr); library(ggplot2); library(rstan); library(reshape2)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


Next we define the model we want to estimate (or in this case, simply run). Note that you can define this as a string, as I have done below, or as a text file that is saved with a `.stan` suffix. The script below is annotated. A few things to notice: 

- Stan writes C++ programs, and so requires static typing (that is, defining the data and variables by their type). 
- All random number generators in Stan are distribution names followed by `_rng`
- A very complete account of the Stan language is available in the [Stan Modeling Language User's Guide and Reference Manual](http://mc-stan.org/documentation/). 


```{r, cache=TRUE}
model_string <- "
data {
    // Define the data
    int N; // the number of observations
    int P; // the number of covariates
    matrix[N, P] X; // The covariate matrix

    // Seeing as we're simulating the model for known parameters, we should pass these too
    vector[P] beta; // The regression coefficients
    real sigma; // The standard deviation of the regression
    real nu; // The degrees of freedom parameter
}
parameters {
  // The parameters we want to estimate would go in here
}
model {
  // This is where the probability model we want to estimate would go
}
generated quantities {
    vector[N] yhat; // Define the outcome variable

    // This is where we generate outcomes for a given draw of parameters and the Xs.
    
    for(i in 1:N){
        yhat[i] <- student_t_rng(nu, X[i,]*beta, sigma);
    }
}
"

```

Next we generate the required inputs in order to generate the data $y$ (that is, seeing as we're drawing from $p(y|\beta, \sigma, X)$, we need values for $\beta, \sigma$ and $X$). We turn these into a data list and compile the Stan script defined above. This should take around 20 seconds (but only needs to be done once). 

```{r, cache = T}
# Generate a matrix of random numbers, and values for beta, nu and sigma

set.seed(42)
N <- 100
P <- 10
X <- matrix(rnorm(N*P), N, P)
nu <- 10
sigma <- 5
beta <- rnorm(10)
# Make sure the first element of beta is positive
beta[1] <- abs(beta[1])


# Put the data into a list
data_list <- list(N = N, P = P, X = X, sigma = sigma, beta = beta)

# Compile the model
compiled_model <- stan_model(model_code = model_string)
```

Finally, we run the script above to generate some plausible values for $\hat{y}$. The output below summarises the "draws" of $\hat{y}$ just as it would parameter estimates from the model. The reason for this will be apparent later on. 

```{r, cache = T, message = F, echo = T}
simulated_values <- sampling(compiled_model, data = data_list, iter = 1, chains = 1,algorithm = "Fixed_param")
simulated_values

```

We can turn this somewhat strange data format into something more useable like so: 

```{r, cache = T, message = F}
yhat <- plyr::ldply(extract(simulated_values, pars = "yhat", permuted = F))$V1

```


### Recovering known parameters from simulated data

Now we have our known parameters `beta`, `nu` and `sigma`, our right-hand-side variables $X$, and our simulated outcomes $y$. Now let's estimate two models to see if we can pin down the known parameters. The two models we'll estimate will be those two models described above--one that assumes normal errors, and the other that assumes Student t errors with a positive $\beta_{1}$. 

First we need to write out the two models to be estimated. These will take a similar form to the script above, although now we will specify the parameters to be estimated and the probability model. 

First we define the incorrectly specified model:

```{r, cache = T}
incorrect_model <- "
data {
  int N; // number of observations
  int P; // number of covariates
  matrix[N, P] X; //covariate matrix
  vector[N] y; //outcome vector
}
parameters {
  vector[P] beta; // the regression coefficients
  real<lower = 0> sigma; // the residual standard deviation (note that it's restricted to be non-negative)
}
model {
  // Define the priors
  beta ~ normal(0, 5); // same prior for all betas; we could define a different one for each, or use a multivariate prior
  sigma ~ cauchy(0, 2.5);

  // The likelihood
  y ~ normal(X*beta, sigma);
}
generated quantities {
  // For model comparison, we'll want to keep the likelihood contribution of each point
  vector[N] log_lik;
  for(i in 1:N){
    log_lik[i] <- normal_log(y[i], X[i,]*beta, sigma);
  }
}
"
```

Next, the correctly specified model:

```{r, cache = T}
correct_model <- "
data {
  int N; // number of observations
  int P; // number of covariates
  matrix[N, P] X; //covariate matrix
  vector[N] y; //outcome vector
}
parameters {
  // We need to define two betas--the first is the restricted value, the next are the others. We'll join these in the next block
  real<lower = 0> beta_1;
  vector[P-1] beta_2; // the regression coefficients
  real<lower = 0> sigma; // the residual standard deviation (note that it's restricted to be non-negative)
  real<lower = 1> nu; 
}
transformed parameters {
  vector[P] beta;
  beta <- append_row(rep_vector(beta_1, 1), beta_2);
}
model {
  // Define the priors
  beta ~ normal(0, 5); // same prior for all betas; we could define a different one for each, or use a multivariate prior. The first beta will have a prior of the N+(0, 5)
  sigma ~ cauchy(0, 2.5);
  nu ~ cauchy(7, 10);

  // The likelihood
  y ~ student_t(nu, X*beta, sigma);
}
generated quantities {
  // For model comparison, we'll want to keep the likelihood contribution of each point
  vector[N] log_lik;
  for(i in 1:N){
    log_lik[i] <- student_t_log(y[i], nu, X[i,]*beta, sigma);
  }
}
"
```

Now estimate the models

```{r, cache = T, warning=F, message=F}

data_list_2 <- list(X = X, N = N, y = yhat, P = P)
incorrect_fit <- stan(model_code = incorrect_model, data = data_list_2, cores = 4, verbose = F)
correct_fit <- stan(model_code = correct_model, data = data_list_2, cores = 4, verbose = F)


```

And print the parameter estimates along side the actuals

```{r, cache = T, warning = F}
print(incorrect_fit, pars = c("beta", "sigma"))
print(correct_fit, pars = c("beta", "sigma", "nu"))
```

Notice that the parameter estimates printed above are in the same format as the simulated data above? When we call `print()` on a `stanfit` object, it generates some summary statistics for all parameters *and generated quantities*. In Markov Chain Monte Carlo MCMC methods, the posterior distribution $p(\theta|y)$ is estimated by generating a large number of draws from the distribution (there is no analytical form from which to calculate the relevant statistics). For each draw of the parameters, values in the `generated quantities` block are created. That is, in each iteration we generate values in the generated quantities block that make use of different parameter estimates. Once we have a large number of draws from the distribution, we can calculate relevant statistics of those draws (`print()` returns mean, standard deviation, and quantiles). But we also want to know if those draws are reliable. 

The `print()` method calculates a couple more useful statistics to help us know when the estimates are unreliable. First is `n_eff`. This tells us the effective number of *independent draws* that we have. Some MCMC algorithms (like Gibbs sampling or Metropolis Hastings, which Stan *does not* implement) are prone to drawing serially correlated estimates of the parameters---one iteration's estimates are highly dependent on the previous iteration's. When this happens, the 